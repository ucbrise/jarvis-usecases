{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Jarvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import re\n",
    "\n",
    "import jarvis\n",
    "jarvis.groundClient('git')\n",
    "jarvis.jarvisFile('logistic_regression.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jarvis.func\n",
    "def crawl():\n",
    "    train = pd.read_csv('train.csv')\n",
    "    train['email'] = train['email'].str.lower()\n",
    "    test = pd.read_csv('test.csv')\n",
    "    test['email'] = test['email'].str.lower()\n",
    "    stop_words = set(pd.read_csv('stopwords.csv')['a'])\n",
    "    return train, test, stop_words\n",
    "\n",
    "doCrawl = jarvis.Action(crawl)\n",
    "train_set = jarvis.Artifact('train_set.pkl', doCrawl)\n",
    "test_set = jarvis.Artifact('test_set.pkl', doCrawl)\n",
    "stopwords = jarvis.Artifact('stop_words.pkl', doCrawl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_in_text(words, text):\n",
    "    return pd.Series([1 if word in text else 0 for word in words])\n",
    "\n",
    "def prop_punc(punc, text):\n",
    "    return np.sum([1 if punc == text[i] else 0 for i in range(len(text))]) / len(text)\n",
    "\n",
    "def proportion_capital(text):\n",
    "    return len([letter for letter in text if letter.isupper()]) / len(text)\n",
    "\n",
    "def count_words(arr, stop_words):\n",
    "    dict = {}\n",
    "    for email in arr:\n",
    "        for word in str(email).split():\n",
    "            if word not in stop_words:\n",
    "                if word in dict:\n",
    "                    dict[word] += 1\n",
    "                else:\n",
    "                    dict[word] = 1\n",
    "    return dict\n",
    "\n",
    "def ngrams(inputs, n):\n",
    "    inputs = inputs.split(' ')\n",
    "    output = {}\n",
    "    for i in range(len(inputs)-n+1):\n",
    "        g = ' '.join(inputs[i:i+n])\n",
    "        output.setdefault(g, 0)\n",
    "        output[g] += 1\n",
    "    return output\n",
    "\n",
    "def grams_in_text(gram_list, text):\n",
    "    text_grams = ngrams(text, 2)\n",
    "    return pd.Series([1 if gram in text_grams else 0 for gram in gram_list])\n",
    "\n",
    "def create_row(table, index, most_distinguished_email, most_distinguished_ngrams, most_distinguished_subject):\n",
    "    row = table.iloc[index, :]\n",
    "    lst = []\n",
    "#     lst.append(re.search(r'R[E|e]:', str(row['subject'])) == None)\n",
    "#     lst.append(re.search(r'F[W|w][D|d]*:', str(row['subject'])) == None)\n",
    "    lst.append(row['reply'])\n",
    "    lst.append(row['forward'])\n",
    "    lst.append(row['length of email'])\n",
    "    lst.append(row['brackets subject'])\n",
    "    for p in ['!', '?', '-', ':', '*', '#', '$']:\n",
    "        lst.append(row['prop ' + p])\n",
    "    words_email = most_distinguished_email\n",
    "    lst += list(words_in_text(words_email, row['email']))\n",
    "    subject_email = most_distinguished_subject[:2]\n",
    "    lst += list(words_in_text(subject_email, np.array(row['subject'])))\n",
    "    grams_email = most_distinguished_ngrams\n",
    "    lst += list(grams_in_text(grams_email, row['email']))\n",
    "    return np.array(lst)\n",
    "    \n",
    "\n",
    "@jarvis.func\n",
    "def featurize(train, test, stop_words):\n",
    "    train['reply'] = train['subject'].apply(lambda x: re.search(r'R[E|e]:', str(x))).apply(lambda x: 0 if x == None else 1)\n",
    "    train['forward'] = train['subject'].apply(lambda x: re.search(r'F[W|w][D|d]*:', str(x))).apply(lambda x: 0 if x == None else 1)\n",
    "    \n",
    "    punc = ['!', '?', '.', '-', ':', '*', '#', '$', '%', '<', '>', '@']\n",
    "    for p in punc:\n",
    "        train['prop ' + p] = train['email'].apply(lambda e: prop_punc(p, e))\n",
    "    \n",
    "    train['capital proportion'] = train['subject'].apply(lambda x: proportion_capital(str(x)))\n",
    "    train['brackets subject'] = train['subject'].apply(lambda x: re.search(r'\\[.*\\]', str(x))).apply(lambda x: 0 if x == None else 1)\n",
    "    train['length of email'] = train['email'].apply(lambda x: len(x))\n",
    "    train['log length'] = np.log(train['length of email'])\n",
    "    spam_words = count_words(np.array(train[train['spam'] == 1]['email']), stop_words)\n",
    "    ham_words = count_words(np.array(train[train['spam'] == 0]['email']), stop_words)\n",
    "    \n",
    "    shorter_spam = sorted(spam_words, key=spam_words.get, reverse=True)[:200]\n",
    "    shorter_ham = sorted(ham_words, key=ham_words.get, reverse=True)[:200]\n",
    "\n",
    "    prop_diff = {}\n",
    "    for word in shorter_spam:\n",
    "        if word in ham_words:\n",
    "            prop_diff[word] = abs(spam_words[word] - ham_words[word])\n",
    "        else:\n",
    "            prop_diff[word] = spam_words[word]\n",
    "    for word in shorter_ham:\n",
    "        if word in spam_words:\n",
    "            prop_diff[word] = abs(spam_words[word] - ham_words[word])\n",
    "        else:\n",
    "            prop_diff[word] = ham_words[word]\n",
    "\n",
    "    mde = sorted(prop_diff, key=prop_diff.get, reverse = True)\n",
    "    \n",
    "    spam_subject = count_words(np.array(train[train['spam'] == 1]['subject']), stop_words)\n",
    "    ham_subject = count_words(np.array(train[train['spam'] == 0]['subject']), stop_words)\n",
    "\n",
    "    shorter_spam_subject = sorted(spam_subject, key=spam_subject.get, reverse=True)[:100]\n",
    "    shorter_ham_subject = sorted(ham_subject, key=ham_subject.get, reverse=True)[:100]\n",
    "\n",
    "    prop_diff_subject = {}\n",
    "    for word in shorter_spam_subject:\n",
    "        if word in ham_subject:\n",
    "            prop_diff_subject[word] = abs(spam_subject[word] - ham_subject[word])\n",
    "        else:\n",
    "            prop_diff_subject[word] = spam_subject[word]\n",
    "    for word in shorter_ham_subject:\n",
    "        if word in spam_subject:\n",
    "            prop_diff_subject[word] = abs(spam_subject[word] - ham_subject[word])\n",
    "        else:\n",
    "            prop_diff_subject[word] = ham_subject[word]\n",
    "\n",
    "    mds = sorted(prop_diff_subject, key=prop_diff_subject.get, reverse = True)\n",
    "    \n",
    "    ham_ngrams = ngrams(train[train['spam'] == 0]['email'].str.cat(sep=\" \"), 2)\n",
    "    spam_ngrams = ngrams(train[train['spam'] == 1]['email'].str.cat(sep=\" \"), 2)\n",
    "\n",
    "\n",
    "    short_spam_ngrams = sorted(spam_ngrams, key=spam_ngrams.get, reverse=True)[:200]\n",
    "    short_ham_ngrams = sorted(ham_ngrams, key=ham_ngrams.get, reverse=True)[:200]\n",
    "\n",
    "    short_spam_ngrams\n",
    "\n",
    "    prop_diff_ngrams = {}\n",
    "    for word in short_spam_ngrams:\n",
    "        if word in ham_ngrams:\n",
    "            prop_diff_ngrams[word] = abs(spam_ngrams[word] - ham_ngrams[word])\n",
    "        else:\n",
    "            prop_diff_ngrams[word] = spam_ngrams[word]\n",
    "    for word in short_ham_ngrams:\n",
    "        if word in shorter_spam:\n",
    "             prop_diff_ngrams[word] = abs(spam_ngrams[word] - ham_ngrams[word])\n",
    "        else:\n",
    "            prop_diff_ngrams[word] = ham_ngrams[word]\n",
    "\n",
    "    mdn = sorted(prop_diff_ngrams, key=prop_diff_ngrams.get, reverse = True)[:200]\n",
    "    \n",
    "    X_train_new = np.array([create_row(train, i, mde, mdn, mds) for i in range(len(train))])\n",
    "    y_train_new = train['spam']\n",
    "\n",
    "    X_test = np.array([create_row(train, i, mde, mdn, mds) for i in range(len(test))])\n",
    "    \n",
    "    return X_train_new, y_train_new, X_test\n",
    "\n",
    "doFeaturize = jarvis.Action(featurize, [train_set, test_set, stopwords])\n",
    "x_tr = jarvis.Artifact('x_tr.pkl', doFeaturize)\n",
    "y_tr = jarvis.Artifact('y_tr.pkl', doFeaturize)\n",
    "x_te = jarvis.Artifact('x_te.pkl', doFeaturize)\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_optimal_cutoff(fpr, tpr, thresholds):\n",
    "        dict = {}\n",
    "        for i in range(len(thresholds)):\n",
    "            dict[thresholds[i]] = tpr[i] - fpr[i]\n",
    "        return sorted(dict, key = dict.get, reverse = True)[0]\n",
    "\n",
    "@jarvis.func\n",
    "def train_model(X_train_new, y_train_new):\n",
    "    clf = LogisticRegressionCV(cv=5)\n",
    "    clf.fit(X_train_new, y_train_new)\n",
    "    y_predicted = clf.predict_proba(X_train_new)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train_new, y_predicted)\n",
    "    optimal_cutoff = find_optimal_cutoff(fpr, tpr, thresholds)\n",
    "    tr_opt_cut = \"Optimal_cutoff: {}\".format(optimal_cutoff)\n",
    "    return clf, tr_opt_cut\n",
    "\n",
    "doTrainModel = jarvis.Action(train_model, [x_tr, y_tr])\n",
    "model = jarvis.Artifact('model.pkl', doTrainModel)\n",
    "opt_cutoff= jarvis.Artifact('optimal_cutoff.txt', doTrainModel)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9900574988021082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@jarvis.func\n",
    "def score_model(model, X_train_new, y_train_new):\n",
    "    tr_acc = \"Train Accuracy: {}\".format(model.score(X_train_new, y_train_new))\n",
    "    return tr_acc\n",
    "\n",
    "doScoreModel = jarvis.Action(score_model, [model, x_tr, y_tr])\n",
    "output = jarvis.Artifact('output.txt', doScoreModel)\n",
    "output.peek(lambda x: print(''.join(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', '1'),\n",
       " ('1', '0'),\n",
       " ('6', '5'),\n",
       " ('5', '3'),\n",
       " ('5', '4'),\n",
       " ('11', '10'),\n",
       " ('10', '7'),\n",
       " ('10', '8'),\n",
       " ('10', '9'),\n",
       " ('16', '15'),\n",
       " ('15', '12'),\n",
       " ('15', '13'),\n",
       " ('15', '14'),\n",
       " ('12', '10'),\n",
       " ('13', '10'),\n",
       " ('14', '10'),\n",
       " ('7', '5'),\n",
       " ('8', '5'),\n",
       " ('3', '1'),\n",
       " ('7', '1'),\n",
       " ('8', '1')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pull()\n",
    "output.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
